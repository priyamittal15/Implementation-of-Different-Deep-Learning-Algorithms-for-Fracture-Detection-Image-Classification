# Implementation-of-Different-Deep-Learning-Algorithms-for-Fracture-Detection-Image-Classification
Using-Deep-Learning-Techniques-perform-Fracture-Detection-Image-Processing Using Different Image Processing techniques Implementing Fracture Detection on X rays Images on 8000 + images of dataset  Description About Project:  Bones are the stiff organs that protect vital organs such as the brain, heart, lungs, and other internal organs in the human body. There are 206 bones in the human body, all of which has different shapes, sizes, and structures. The femur bones are the largest, and the auditory ossicles are the smallest. Humans suffer from bone fractures on a regular basis. Bone fractures can happen as a result of an accident or any other situation in which the bones are put under a lot of pressure. Oblique, complex, comminute, spiral, greenstick, and transverse bone fractures are among the many forms that can occur. X-ray, computed tomography (CT), magnetic resonance imaging (MRI), ultrasound, and other types of medical imaging techniques are available to detect various types of disorders. So we design the architecture of it using Neural Networks different models, compare the accuracy, and get a result of which model works better for our dataset and which model delivers correct results on a specific related dataset with 10 classes. Basically our main motive is to check that which model works better on our dataset so in future reference we all get an idea that which model gives better type of accuracy for a respective dataset . 

Proposed Method for Project:  

we decided to make this project because we have seen a lot of times that report that are generated by computer produce error sometimes so we wanted to find out which model gives good accuracy and produce less error so we start to research over image processing nd those libraries which are used in image processing like Keras , Matplot lib , Image Generator , tensor flow and other libraries and used some of them and implement it on different image processing algorithm like as CNN , VGG-16 Model ,ResNet50 Model , InceptionV3 Model . and then find the best model which gives best accuracy for that we generate classification report using predefined libraries in python such as precision , recall ,r2score , mean square error etc by importing Sklearn.  

Methodology of Project:  
Phase 1: Requirement analysis:  
• Study concepts of Basic Python programming. 
• Study of Tensor flow, keras and Python API interface . 
• Study of basic algorithms of Image Processing and neural network And deep learning concepts.
• Collect the dataset from different resources and describe it into Different classes(5 Fractured + 5 non fractured).  

Phase 2: Designing and development:  The stages of design and development are further segmented. This step starts with data from the Requirement and Analysis phase, which will lead to the model construction phase, where a model will be created and an algorithm will be devised. After the algorithm design phase is completed, the focus will shift to algorithm analysis and implementation in this project.  

Phase 3: Coding Phase:  Before real coding begins, the task is divided into modules/units and assigned to team members once the system design papers are received. Because code is developed during this phase, it is the developers' primary emphasis. The most time-consuming aspect of the project will be this. This project's implementation begins with the development of a program in the relevant programming language and the production of an error-free executable program. 

Phase 4: Testing Phase:  When it comes to the testing phase, we may test our model based on the classification report it generates, which contains a variety of factors such as accuracy, f1score, precision, and recall, and we can also test our model based on its training and testing accuracy. 

Phase 5: Deployment Phase:  One of our goals is to bring all of the previous steps together and put them into practice. Another goal is to deploy our model into a python-based interface application after comparing the classification reports and determining which model is best for our dataset.
